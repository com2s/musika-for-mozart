---
# Musika 사용법

https://github.com/marcoppasini/musika

위 링크 참고

ffmpeg 오류 발생 시 아래 코드로 패키지 설치

> conda install -c conda-forge ffmpeg

아래 코드로 우리의 결과를 보실 수 있습니다. (app.py 주소 확인)

> python app.py --base_channels 256 

위 모델은 피아노 음악을 생성하는 모델로 특히 모차르트풍의 곡을 만들어냅니다.  
3가지 모델이 준비되어 있으며 피아노 음색과 코드를 생성하는 사전학습모델, 모차르트의 곡만으로 학습된 모델, 사전학습모델에 모차르트의 곡을 파인튜닝한 모델입니다.  

진행한 실험들에 대한 좀 더 자세한 내용들은 experiment.txt 파일을 참고해주시기 바랍니다.

---

# 프로젝트 결론

우리는 이번 실험을 통해 원하는 음악을 생성하기 위해 어떤 모델이 적합한지 조사하고 Musika 모델로부터 더 나은 생성물을 얻기 위해 노력했다.  적합한 모델을 찾는 과정에서의 주요 쟁점은 Jukebox와 Musika 중 무엇이 우리의 목적에 더 부합하는가에 대한 것이었다. 높은 수준의 오디오를 만들어내고, 사람의 목소리를 재현하는 능력이 뛰어나며, 각종 조건들을 컨디셔닝할 수 있는 Jukebox는 매력적인 모델이었다. 그에 비해 Musika는 오디오의 품질도 더 낮았고 조건들을 컨디셔닝할 수도 없었다. 그럼에도 불구하고 Jukebox는 최종적으로 우리의 고려대상에서 제외되었다. 그 이유는 바로 Jukebox 모델이 요구하는 엄청난 컴퓨터 자원과 시간 때문이었다. 우리는 2가지 환경에서 주로 작업했는데 하나는 RTX 2070을 사용하는 노트북이었고 또 다른 하나는 V100을 사용하는 GCP였다. 전자의 경우 Jukebox 모델을 학습하기는 커녕 오디오를 생성하는 것조차 불가능에 가까웠다. RTX 2070보다 더욱 빠른 속도를 보여준  V100의 GCP 역시 고작 몇 분의 오디오를 생성하기 위해 10~20시간 가량이 소요되었다.

 그러나 Musika는 작동 시간이 압도적으로 빨랐다. 모델을 학습시키는데 필요한 시간은 RTX 2070 하에서 기본값으로 진행하면 1에폭 당 15분 정도였다. Base channels를 2배로 늘려도 30분이었다. 속도가 더 빠른 GCP에서는 이 시간이 절반으로 단축되었다. 오디오를 학습하기 위해 필요한 시간이 다른 태스크들에 비해 매우 긴 편에 속한다는 점을 고려했을 때 이는 충분히 빠른 속도였다. 오디오를 생성하는 속도는 더욱 더 주목할만했다. 1분 58초의 음악을 생성하는 데에 환경에 따라 다르지만 작업을 위해 준비한 RTX 2070의 노트북에서 5초 이내로 걸렸으며 일반적인 용도로 사용하는 데스크탑도 30초 정도면 충분했다. 이 압도적인 시간 차이는 생성물의 열세를 뒤집기에 충분했다. 사람들이 쉽게 접근해 사용하기 위해서는 생성 속도가 빠를수록 용이하기 때문이다.

 우리는 Musika를 사용하기로 한 후 피아노 음악을 만드는 모델을 만들기로 결정했다. 피아노는 사람들에게 가장 익숙한 기본적인 악기이기 때문이다. 또한 모델의 변화를 명확하게 파악하기 위해서는 다양한 악기의 음색이 포함된 복잡한 곡들보다는 피아노 단일로 이루어진 곡들만을 사용해 학습과 생성을 하는 것이 더 나을 것이라 생각했다.

 이후 이어진 각종 실험을 통해 진행 과정에서 가장 큰 영향을 미치는 몇 가지 요소들을 찾을 수 있었다. 그것들은 바로 데이터셋, learning rate, epochs, base channels (파라미터의 갯수), 파인튜닝 여부였다.

 데이터셋을 잘 구성하는 것은 이후 소리의 모든 부분에 관여하므로 매우 중요했다. 더 좋은 음질과 음색을 재현해내기 위해서는 데이터셋 내의 음악들이 일관성을 가지고 있어야한다. 그 일관성이란 사용하는 악기, 음악 스타일 등이 모두 포함된다. 그러나 너무 일관적이라면 생성물도 그 틀을 거의 벗어나지 않게 될 것이므로 주의해야한다. 또 하나의 주의점은 인코딩 시 음질의 열화가 발생할 수 있다는 점이다. 추측하기로는 지나치게 긴 길이의 음악 파일을 인코더에 넣을 경우 특히 심해진다고 본다. 웬만하면 파일 당 10분 이내로 하고 30분이 넘어가는 파일은 미리 분할하는 것이 좋아보인다.

 학습률, learning rate는 어느 정도 높으면 학습 도중 loss에서 에러가 발생해 더 이상 정상적인 진행이 불가능해진다. 이로 인해 우리는 0.00004를 기본적으로 썼으며 base channels 값을 늘릴 땐 그에 반비례하여 더 작은 값을 사용했다. 그럼에도 불구하고 에러가 발생할 위험이 있기 때문에 체크포인트에 저장되는 폴더를 잘 살펴봐야한다. 물론 학습률을 더 낮게 한다면 에러가 발생할 위험은 크게 떨어지겠지만 그렇게 할 경우 학습에 더 많은 시간이 필요해질 것이다.

 Epochs는 기본값이 250회로 되어있으며 1에폭 당 약 9300이터만큼 학습이 진행된다. 경험상 에폭이 진행될수록 점점 음색과 음질이 눈에 띄게 향상되어갔다. 이런 기조는 약 300~400에폭까지는 유지되므로 만약 추가 학습을 진행한다면 더 나은 결과가 나올 것이다. 그러나 500에폭 정도가 되면 학습을 진행하는 데 들이는 시간에 비해 모델이 거의 변하지 않게 되므로 적절한 수준으로 두는 것이 좋다. 기본값인 250으로도 충분한 결과가 나왔으므로 필요한 경우에만 추가학습이 권장한다.

 Base channels는 GAN 아키텍처가 갖는 파라미터의 갯수이다. 즉 생성자와 판별자 모두의 파라미터가 이 값을 따른다. 당연히 모델의 파라미터가 늘어나면 그에 비례하여 학습에 필요한 시간이 증가한다. 안 그래도 오래 걸리는 학습 시간이 더 걸리게 되므로 이 값은 마지막에 늘리는 것을 권장한다. 더 많은 시간이 필요한 만큼 성능의 향상에 있어서 분명 도움이 되었다. 이 값을 2배로 늘리고 진행한 실험에서는 모든 부분에 있어서 질적으로 향상된 생성물을 확인할 수 있었다.

 파인튜닝은 우리에게 꽤나 효과적인 방법이었다. 우선 피아노의 음색 재현에 특화되어있는 데이터셋으로 모델을 미리 학습시켜두었다. 이렇게 만들어진 사전 학습 모델은 음악적으로 뛰어난 멜로디를 만들어내지는 못 하지만 불협화음 없는 코드 구성과 뛰어난 피아노 재현 능력을 보여주었다. 이것을 가지고 특정한 스타일의 음악만을 따로 모아둔 데이터셋으로 파인튜닝을 진행하면 (100에폭 이상을 권장하는데 에폭이 낮으면 음색의 수준도 낮아진다.) 원하는 결과물을 손쉽게 얻을 수 있게 되는 것이다.

 이렇게 지금까지 음악을 wave로 생성하는 모델들에 대해 찾아보고 Musika를 더 효율적으로 학습시키는 방법에 대해 알아보았으며 쉽게 사용 가능한 피아노 생성 모델을 만들어 보았다. 데이터셋만 다르게 준비한다면 피아노 뿐만 아니라 다른 악기의 소리, 합주, 또는 전자음 등 역시 재현할 수 있을 것이다. 우리가 만든 모델이 사람들에게 재미와 음악에 대한 흥미를 유발하고 또 누군가에게는 새로운 음악을 창작하는데 도움이 되기를 바란다. 그리고 역시 자기만의 모델을 만들고자 하는 사람이 있다면 그들에게 우리의 여정이 좋은 참고가 되기를 바란다.
